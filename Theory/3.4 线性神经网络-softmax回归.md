+ 写在前面
事实上，我们不仅对回归问题感兴趣，也对分类问题感兴趣。
1、我们只对样本的“硬性”类别感兴趣，即属于哪个类
2、我们仍希望得到“软性”类别，即得到属于每个类别的概率
注意：即使我们只关心硬类别，我们仍然使用软类别的模型

## 3.4.1 分类问题
我们从一个图像分类问题开始。 假设每次输入是一个 2x2的灰度图像。 我们可以用一个标量表示每个像素值，每个图像对应四个特征 $x_1, x_2, x_3, x_4$ 。 此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。

接下来，我们要选择如何表示标签。 我们有两个明显的选择：最直接的想法是选择 $y\in{1,2,3}$ ， 其中整数分别代表狗猫鸡
。 这是在计算机上存储此类信息的有效方法。 如果类别间有一些自然顺序， 比如说我们试图预测婴儿儿童青少年青年人中年人老年人 ${婴儿，儿童，青少年，青年人，中年人，老年人}$ ， 那么将这个问题转变为回归问题，并且保留这种格式是有意义的。

但是一般的分类问题并不与类别之间的自然顺序有关。 幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：独热编码（one-hot encoding）。 独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。 在我们的例子中，标签 $y$ 将是一个三维向量， 其中 $(1,0,0)$ 对应于“猫”、 $(0,1,0)$ 对应于“鸡”、 $(0,0,1)$ 对应于“狗”： $$y\in{(1,0,0),(0,1,0),(0,0,1)}$$

## 3.4.2 网络架构
为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。
 $o_1 = x_1w_{11}+x_2w_{1,2}+x_3_w_{1,3}+x_4w_{1,4}$
 $o_2 = x_1w_{21}+x_2w_{2,2}+x_3_w_{2,3}+x_4w_{2,4}$
 $o_3 = x_1w_{31}+x_2w_{3,2}+x_3_w_{3,3}+x_4w_{3,4}$
与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出 $o_1$ 、 $o_2$ 和 $o_3$ 取决于所有输入 $x_1、x_2、x_3、x_4$ ， 所以softmax回归的输出层也是全连接层。

## 3.4.3 全连接层的参数开销
具体来说，任何具有 d 个输入 q 个输出的全连接层，参数开销都为 d×q。

## 3.4.4 softmax 运算
softmax 计算公式如下
 $y^{hat}=softmax(o),其中 y_j^{hat}=\frac{exp(o_j)}{\sum_k{exp(o_k)}}$
这里对所有的 j 总有一个 $y_j$ 。因此可以视 $y^{hat}$ 为一个正确的概率分布。
尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个线性模型（linear model）。

## 3.4.6 损失函数
用一个损失函数来度量预测的效果。我们将使用最大似然估计。

### 3.4.6.1 对数似然
softmax 函数给出了一个向量 $\hat{y}$ ，我们可以将其视为“对给定任意输入 x 的每个类的条件概率”。 例如， $\hat{y}=P(y=猫|x)$ 。假设整个数据集 {X，Y} 具有 n 个样本，其中索引 i 的样本由特征向量 $x^{(i)}$ 和独热标签向量 $y^{(i)}$ 组成。我们可以将估计值于实际值进行比较：
 $$P(Y|X)=\prod_{i=1}^n{P(y^{(i)}|x^{(i)})}$$ 
根据最大似然估计，我们最大化 P(Y|X)，相当于最小化负对数似然：
 $$-logP(Y|X)=\sum_{i=1}^n{-logP(y^{(i)}|x^{(i)})}=\sum_{i=1}^n{l(y^{(i)},\hat{y}^{(i)})}$$
其中对于任何标签 y 和模型预测 $\hat{y}$ ，损失函数为：$ l(y,\hat{y})=-\sum_{j=1}^q{y_jlog\hat{y}_j}$

### 3.4.6.2 softmax及其导数
### 3.4.6.3 交叉熵巡视

## 3.4.7 信息论基础
信息熵设计编码、解码、发送以及尽可能简洁的处理信息或数据
### 3.4.7.1 熵
 $H[P]=\sum_j{-P(j)log(P_j)}$

### 3.4.7.2 信息量
压缩与预测有什么关系呢？ 想象一下，我们有一个要压缩的数据流。 如果我们很容易预测下一个数据，那么这个数据就很容易压缩。 为什么呢？ 举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。 由于它们总是相同的，我们总是知道下一个数据是什么。 所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到”惊异”。 克劳德·香农决定用信息量 $log{\frac{1}{P(j)}=-log{P(j)}}$ 来量化这种惊异程度。 在观察一个事件 j 时，并赋予它（主观）概率 P(j) 。 当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。 定义的熵中， 是当分配的概率真正匹配数据生成过程时的信息量的期望。

### 3.4.7.3 重新审视交叉熵
如果把熵 H(P) 想象为“知道真实概率的人所经历的惊讶程度”，那么什么是交叉熵？交叉熵从 P 到 Q， 记为 H(P,Q) 。我们可以把交叉熵想象为“主观观测概率为 Q 的观察者在看到根据概率 P 生成的数据时的预期惊异”。当 P=Q时， 交叉熵达到最低。这种情况下，从 P 到 Q 的交叉熵是 H(P,P)=H(P).
简而言之，从两方面考虑交叉熵分类目标：
+ 最大化观测数的似然
+ 最小化传递标签所需的惊异