```text
数据主要分为两种，一种是张量，一种是矢量
GPU支持矢量加速计算，Numpy仅支持CPU计算
张量支持自动微分
```
# 2.1.1
张量的使用我们在 tensorflow 下通常要导入包才可以使用
```python
import tensorflow as tf
```
张量是由数值组成的数组，这个数组可以有多个维度。具有一个轴的张量对应数学上的向量 vector ；两个轴对应矩阵 matrix ；两个轴以上的没有特殊的名字。

可以用range创建一个行向量 x 。默认为整数，也可以指定创建为浮点数。 张量中每个值称为 元素 element。除非额外指定，新的张量将存储在内存中，并采用CPU计算。
```python
x = tf.range(12)
x
```
```
<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)>
```

查看 x 的形状
```python 
x.shape
# TensorShape([12])
```

如果想知道张量中的元素总数，即形状的所有元素乘积，可以检查他的大小。
```python 
tf.size()
# <tf.Tensor: shape=(), dtype=int32, numpy=12>
```

想更改张量的形状而不改变元素数量和元素值，可以用 reshape 函数。
```python
x = tf.reshape(x, (3,4))
x
'''
 <tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]], dtype=int32)>
'''
```

也可以创建全0或者全1或者其他常量
```python
tf.zeros((2,3,4))
tf.ones((2,3,4))
tf.random.normal(shape=[3, 4]) # 已某种分布来进行随机采样
```

另一种方式就是通过 python 列表，最外层的列表对应于轴0，内层的列表对应轴1
```python
tf.constant([[2,1,4,3], [3,4,2,1], [4,5,2,1]])
```

## 2.1.2 运算符
我们不仅仅是为了读写数据，跟是想用数据进行操作。
```
x = tf.constant([1.0, 2, 4, 8])
y = tf.constant([2.0, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y
# output
(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 3.,  4.,  6., 10.], dtype=float32)>,
 <tf.Tensor: shape=(4,), dtype=float32, numpy=array([-1.,  0.,  2.,  6.], dtype=float32)>,
 <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 2.,  4.,  8., 16.], dtype=float32)>,
 <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.5, 1. , 2. , 4. ], dtype=float32)>,
 <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 1.,  4., 16., 64.], dtype=float32)>)
```

还可以“按元素”求幂
```
tf.exp(x)
# output
<tf.Tensor: shape=(4,), dtype=float32, numpy=
array([2.7182817e+00, 7.3890562e+00, 5.4598148e+01, 2.9809580e+03],
      dtype=float32)>
```

除了上述元素计算外，我们还可以计算线性代数，包括向量点积和矩阵乘法。
我们也可以把多个张量连在一起得到一个更大的张量。$\color{blue}{注意：类型必须相同}$
```
x = tf.reshape(tf.range(12, dtype=tf.float32), (3,4))
y = tf.constant([[2.,1,4,3], [1,2,3,4], [3,4,2,1]])
tf.concat([x,y], axis=0)
tf.concat([x,y], axis=1)
# output
(<tf.Tensor: shape=(6, 4), dtype=float32, numpy=
 array([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [ 2.,  1.,  4.,  3.],
        [ 1.,  2.,  3.,  4.],
        [ 3.,  4.,  2.,  1.]], dtype=float32)>,
 <tf.Tensor: shape=(3, 8), dtype=float32, numpy=
 array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
        [ 8.,  9., 10., 11.,  3.,  4.,  2.,  1.]], dtype=float32)>)
```

对所有元素求和，得到一个单元素的张量$\color{blue}{注意：经常用在}$
```
tf.reduce_sum(x)
# output
<tf.Tensor: shape=(), dtype=float32, numpy=66.0>
```

## 2.1.3 广播机制
广播机制工作方式
1、适当复制元素来扩展一个或两个数组，以便转换后，两个张量具有相同的形状
2、对生成的数组按元素操作
演示第一种
```
a = tf.reshape(tf.range(3), (3, 1))
b = tf.reshape(tf.range(2), (1, 2))
a+b
# output
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[0, 1],
       [1, 2],
       [2, 3]])>
```
可以看到，a 和 b的形状不匹配，将两个矩阵广播为一个更大的3×2矩阵，然后在按元素相加

## 2.1.4 切片和索引
和 python 其他数组一样，张量中的元素也可以通过索引访问。
```python
x[-1], x[1:3]
```
tf 中的 Tensors 是不可变的，也不能被赋值。tf 中 Variables 是支持赋值的可变容器。但是 tf 中的梯度不会通过 Varibales 反向传播。
除了为整个Variable分配一个值之外，我们还可以通过索引来写入Variable的元素。
```python
x_var = tf.Variable(x)
x_var[1,2].assign(9)
x_var
# output
<tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  9.,  7.],
       [ 8.,  9., 10., 11.]], dtype=float32)>
```

## 2.1.5 节省内存
运行一切操作可能导致为新结果分配内存。例如，用y = x + y，我们将取消引用 y 指向的张量，而指向新分配的内存的张量。
```
before = id(y)
y = y+x
id(y) == before
# output
False
```
这可能是不可取的，原因有两个：
1、我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
2、如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

Variables 是 tf 中的可变容器，他们提供一种存储模型参数的方法。我们可以通过 assign 将一个操作的结果分配给一个 Variables。 为了说明这一点，我们创建了一个与另一个张量 y 相同的形状 z。
```python
z = tf.Variable(tf.zeros_like(y))
print(id(z))
z.assign(x + y)
print(id(z))
# output
1675017582152
1675017582152
```
即使你将状态持久存储在Variable中， 你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。

由于TensorFlow的Tensors是不可变的，而且梯度不会通过Variable流动， 因此TensorFlow没有提供一种明确的方式来原地运行单个操作。

但是，TensorFlow提供了tf.function修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销。
```python
@tf.function
def computation(X, Y):
    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除
    A = X + Y  # 当不再需要时，分配将被复用
    B = A + Y
    C = B + Y
    return C + Y

computation(X, Y)
```

## 2.1.6 转换为其他python对象
深度学习框架定义的张量转为 Numpy 张量很容易，反之也很容易。转换后的结果不共享内存。这个小的不便其实非常重要：当在 CPU 和 GPU 上执行操作的时候，如果 python 和 numpy也希望使用相同的内存块执行其他操作，认们不希望停下来计算来等他。
```python
a = x.numpy()
b = tf.constant(a)
type(a), type(b)