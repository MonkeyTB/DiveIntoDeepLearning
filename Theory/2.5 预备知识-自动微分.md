+ 写在前面
求导是几乎所有深度学习算法优化的关键的步骤
深度学习框架设计好的自动计算导数，即自动微分来加快求导，实际中，设计好模型，系统会构建一个计算图来跟踪计算哪些数据通过操作组合产生输出。自动微分使系统能够随后反向传播梯度。

## 2.5.1 一个简单的例子
对函数 $y=2x^Tx$ 关于列向量 x 求导。首先创建 x 并分配初始值
```python
import tensorflow as tf
x = tf.range(4, dtype=tf.float32)
# output
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>
```
我们计算 y 关于 x 的梯度之前，需要一个地方来存储梯度，重要的是，我们不会在每次对一个参数求导时都分配新的内存。因我们经常成千上万次更新相同的参数，每次都分配可能导致内存耗尽。注意，一个标量函数关于向量 x 的梯度是向量，并且于 x 就有相同的形状。
```python
x = tf.Variable(x)
# output
<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>
```
计算 y
```python
with tf.GradientTape() as t:
    y = 2 * tf.tensordot(x, x, axes=1)
# output
tf.Tensor(28.0, shape=(), dtype=float32)
```
x 是一个长度为4的向量，计算 x 和 x 的点积，得到了我们赋值给 y 的标量输出。接下来，通过调用反向传播函数自动计算 y 关于 x 每个分量的梯度，并打印
```python
x_grad = t.gradient(y, x)
x_grad
# output
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>
```
函数$y=2x^Tx$关于x的梯度应为 4x 。
```python
x_grad == 4 * x
# output 
<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>
```
现在计算 x 的另一个函数
```python
with tf.GradientTape() as t:
    y = tf.reduce_sum(x)
t.gradient(y, x) # 被新计算的梯度覆盖
# output
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>
```

## 2.5.2 非标量变量的反向传播
当 y 不是标量时，向量 y 关于向量 x 的导数的最自然解释是一个矩阵。对于高阶和高维的 y 和 x ，求导的结果可以是一个高阶张量。
```python
with tf.GradientTape() as t:
    y = x * x
t.gradient(y,x)
# output
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>
```

## 2.5.3 分离计算
有时我们希望将某些计算移动到记录的计算图之外。 例如，假设 y 是作为 x 的函数计算的，而 z 则是作为 y 和 x 的函数计算的。想想一下，计算 z 关于 x 的梯度，但由于某些原因，希望将 y 视为一个常数， 并且只考虑 x 在 y被计算后发挥的作用。
这里可以分离 y 来返回一个新变量 u， 该变量于 y 具有相同的值，但丢弃计算图中如何计算 y 的任何信息。换句话说，梯度不会后流经 u 到 x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。
```python 
# 设置persistent=True来运行t.gradient多次
with tf.GradientTape(persistent=True) as t:
    y = x * x
    u = tf.stop_gradient(y)
    z = u * x
x_grad = tf.gradient(z, x)
x_grad == u
# output 
<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>
```
由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x
```python
t.gradient(y, x) == 2 * x
# output
<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>
```

## 2.5.4 python 控制流的梯度计算
使用自动微分的一个好处是：即使构建函数的计算图需要通过 python 控制流，我们仍然可以计算得到的变量的梯度。在下面的代码中，while 循环的迭代次数和 if 语句的结果都取决于输入 a 的值。
```python
def f(a):
    b = a ** 2
    while tf.norm(b) < 1000:
        b = b * 2
    if tf.reduce_sum(b) > 0:
        c = b
    else:
        c = 100 * b
    return c

a = tf.Variabel(tf.random.normal(shape()))
with tf.GradientTape() as t:
    d = f(a)
d_grad = t.gradient(d, a)
d_grad
# output
<tf.Tensor: shape=(), dtype=float32, numpy=-2900.877>
```
我们现在可以分析上面定义的f函数。 请注意，它在其输入 a 中是分段线性的。 换言之，对于任何 a，存在某个常量标量k，使得 f(a)=k*a，其中 k 的值取决于输入 a ，因此可以用 d/a 验证梯度是否正确。