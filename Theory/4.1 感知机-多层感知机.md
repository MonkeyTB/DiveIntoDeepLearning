+ 写在前面
第三章介绍了 softmax 回归，本章开始介绍深度神经网络。

## 4.1.1 隐藏层
在第三章中描述了仿射变换，他是一种带偏置项的线性变换。但是仿射变换中的线性是一个很强的假设。

### 4.1.1.1 线性模型可能会出错
线性意味着单调假设：任何特征的增大都会导致模型输出的增大（如果权重为正），或者导致模型的输出的减小（如果权重为负）。 例如，如果我们试图预测一个人是否会偿还贷款。 我们可以认为，在其他条件不变的情况下， 收入较高的申请人比收入较低的申请人更有可能偿还贷款。 但是，虽然收入与还款概率存在单调性，但它们不是线性相关的。 收入从0增加到5万，可能比从100万增加到105万带来更大的还款可能性。 处理这一问题的一种方法是对我们的数据进行预处理， 使线性变得更合理，如使用收入的对数作为我们的特征。

然而我们可以很容易找出违反单调性的例子。 例如，我们想要根据体温预测死亡率。 对体温高于37摄氏度的人来说，温度越高风险越大。 然而，对体温低于37摄氏度的人来说，温度越高风险就越低。 在这种情况下，我们也可以通过一些巧妙的预处理来解决问题。 例如，我们可以使用与37摄氏度的距离作为特征。

### 4.1.1.2 在网络中加入隐藏层
通过对网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普通的函数关系类型。要做到这一点，最简单的方法就是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。这种架构一般称为多层感知机（multilayer perceptron）MLP。

### 4.1.1.3 从线性到非线性
上面提到的加入隐藏层（一个多个），其实整个结构仍然是线性结构，仍然是一个仿射变换。因此为了改变这一点，我们在隐藏层中间引入非线性变化的激活函数 $\sigmoid$ 。这时候，就不再是线性变换了。

## 4.1.2 激活函数
激活函数通过计算加权和并加上偏置来确定神经元是否应该被激活，它将输入信号转换为输出的可微运算。大多激活函数都是非线性的。


### 4.1.2.1 ReLU 函数
$ReLU(x) = max(x,0)$ 
简单来讲 ReLU就是将低于 0 的元素都设为 0， 高于 0 的元素保留。
使用其原因是因为求导表现特别好，要么让参数通过，要么让参数消失，使得优化表现更好，减轻了神经网络梯度消失问题。

### 4.1.2.2 sigmoid 函数
 $sigmoid(x)=\fran{1}{1+exp(-x)}$ 
sigmoid 将输入变换为区间 (0-1) 上的输出。因此将 sigmoid 称为挤压函数。
当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）。 然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中，我们将描述利用sigmoid单元来控制时序信息流的架构。

### 4.1.2.3 tanh 函数
 $tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}$ 
