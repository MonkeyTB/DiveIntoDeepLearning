+ 写在开始
线代知识的了解有利于对于模型的学习

## 2.3.1 标量
在2.1里面已经有了基本的介绍，不再赘述，就是张量的加减乘除

## 2.3.2 向量
向量可以被视为标量值组成的列表。这些标量值被称为向量的元素（element）或者分量（component）。当向量表示数据集中的样本时，他的值具有一定的现实意义。
通常一维张量表示向量。理论上可以任意长度。
```python
x = tf.range(4)
x
# output
<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3])>
```
注意，大部分文献或者本文中，都默认列向量为默认方向
$$
x = [x_1 \\ x_2 \\ x_3 \\ x_4]
$$
其中$x_1, x_2, ... $是向量的元素，在代码中可以通过索引来进行访问
```python
x[3]
```

### 2.3.2.1 长度、维度、形状
```python
len(x) # 4
x.shape # TensorShape([4])
```

## 2.3.3 矩阵
向量将标量从零阶推广到一阶，矩阵是将向量从一阶推广到二阶。矩阵通常用粗体、大写字母表示。
```python
A = tf.reshape(tf.range(20), (5,4))
A
# output
<tf.Tensor: shape=(5, 4), dtype=int32, numpy=
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15],
       [16, 17, 18, 19]])>
```

一般可以通过索引行和列
```python
A[:,0] # <tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  4,  8, 12, 16])>
A[0]   # <tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3])>
```

对 A 进行转置操作
```python
tf.transpose(A)
# output
<tf.Tensor: shape=(4, 5), dtype=int32, numpy=
array([[ 0,  4,  8, 12, 16],
       [ 1,  5,  9, 13, 17],
       [ 2,  6, 10, 14, 18],
       [ 3,  7, 11, 15, 19]])>
```

## 2.3.4 张量
这里和2.3.3提到的矩阵一样，没有什么特别的区别

## 2.3.5 张量算法的基本性质
```python
A = tf.reshape(tf.range(20, dtype=tf.float32), (5,4))
B = A
A, A+B
# output
(<tf.Tensor: shape=(5, 4), dtype=float32, numpy=
 array([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]], dtype=float32)>,
 <tf.Tensor: shape=(5, 4), dtype=float32, numpy=
 array([[ 0.,  2.,  4.,  6.],
        [ 8., 10., 12., 14.],
        [16., 18., 20., 22.],
        [24., 26., 28., 30.],
        [32., 34., 36., 38.]], dtype=float32)>)
```
这里 id(A) == id(B) 返回 True， 也就说明了不能通过分配新内存将 A 克隆 B

两个矩阵的按元素乘积称为 Hadamard 积
$$A\otimes B=\begin{bmatrix}a_{11}b_{11}\ ...\ a_{1n}b_{1n} \\ a_{21}b_{21}\ ...\ a_{2n}b_{2n} \\...\ ...\ ...\ ...\\a_{m1}b_{m1}\ ... a_{mn}b_{mn}\end{bmatrix}$$
```python
A * B
# output
<tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[  0.,   1.,   4.,   9.],
       [ 16.,  25.,  36.,  49.],
       [ 64.,  81., 100., 121.],
       [144., 169., 196., 225.],
       [256., 289., 324., 361.]], dtype=float32)>
```

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

## 2.3.6 降维
我们可以对任意张量进行有用的操作是计算其元素之和。
```python
x = tf.range(4, dtype=tf.float32)
x, tf.reduce_sum(x)
```

也可以对矩阵进行该操作
```python
A.shape, tf.reduce_sum(A)
```

在默认情况下，调用求和函数会沿所有轴降低张量的维度，使它变为一个矢量，我们还可以指定轴的方向进行计算，得到的仍是一个张量
```text
A = <tf.Tensor: shape=(5, 4), dtype=float32, numpy=
    array([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]], dtype=float32)>
```
```python
A_sum_axis0 = tf.reduce_sum(A, axis=0)
A_sum_axis0, A.sum_axis0.shape
# output
(<tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)>,
 TensorShape([4]))
```
同理，axis=1 是对列进行操作
```
<tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)>
```
还可以同时按行和列进行操作(axis=[0, 1])，等同于tf.readuce_sum()

### 2.3.6.1 非降维求和
在调用函数来计算总和或者均值的时候保持轴数不变会非常有用
```python
sum_A = tf.reduce_sum(A, axis=1, keepdims=True)
sum_A
# output
<tf.Tensor: shape=(5, 1), dtype=float32, numpy=
array([[ 6.],
       [22.],
       [38.],
       [54.],
       [70.]], dtype=float32)>
```
例如，由于 sum_A 在每行进行求和后仍保持两个轴，我们可以通过广播将 A 除以 sum_A
```python
A / sum_A
```

可以计算 A 沿某个轴的累计总和，可以用如下方法
```python
tf.cumsum(A, axis=0)
# output
<tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  6.,  8., 10.],
       [12., 15., 18., 21.],
       [24., 28., 32., 36.],
       [40., 45., 50., 55.]], dtype=float32)>
```

## 2.3.7 点积（Dot porduct）
上面介绍了按元素操作、求和和平均值。另一个基本操作就是点积——就是按位置元素乘积求和的结果
```python
y = tf.ones(4, dtype=tf.float32)
x, y, tf.tensordot(x, y, axes=1)
# output
(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3])>,
 <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 1, 1, 1])>,
 <tf.Tensor: shape=(), dtype=int32, numpy=6>)
 ```
 也可以直接用 tf.reduce_sum(x * y)

## 2.3.8 矩阵-向量积
用公式来表示就是$Ax=[A]x=[a_1^Tx, a_2^Tx,...,a_m^Tx]^T$
```python
x = tf.range(4, dtype=tf.float32)
A.shape, x.shape, tf.linalg.matvec(A, x)
# output
(TensorShape([5, 4]),
 TensorShape([4]),
 <tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 14.,  38.,  62.,  86., 110.], dtype=float32)>)
```

## 2.3.9 矩阵-矩阵乘法
矩阵乘法其实就是第一个矩阵的行乘以第一个矩阵的列，作为结果矩阵的第一行第一列的值
```python
B = tf.ones((4,3), tf.float32)
tf.matmul(A, B)
# output 
<tf.Tensor: shape=(5, 3), dtype=float32, numpy=
array([[ 6.,  6.,  6.],
       [22., 22., 22.],
       [38., 38., 38.],
       [54., 54., 54.],
       [70., 70., 70.]], dtype=float32)>
```

## 2.3.10 范数
线代里面有个运算符“范数”（norm）。非正式的说，向量的范数表示一个向量有多大——分量的大小
范数的第一个性质是$f(\alpha\ x)=|\alpha|f(x)$
范数的第二个性质是$f(x+y)\leq\ f(x)+f(y)$
范数的第三个性质是$f(x)\ \geq 0$
```python
# L2范式
u = tf.constant([3,-4])
tf.norm(u)
# L1范式
tf.reduce_sum(tf.abs(u))
```
