+ 写在前面
回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。

## 3.1.1 线性回归的基本元素
线性回归假设：
+ 自变量 x 和因变量 y 之间的关系是线性的
+ 任何噪声都是合理的，比如噪声遵循正态分布
比如根据房屋面积和房龄估算房屋价格

### 3.1.1.1 线性模型
 $price=w_{area}·area+w_{age}·age+b$ 
其中 w 称为权重，权重决定了每个特征对我们预测值的影响。b 表示偏置，偏置代表所有特征都为 0 的时候，预测值应该为多少。更深层一点说是为了提高模型的表达能力。

机器学习中，我们特征长使用高维， $y_hat = w_1x_1+w_2x_2+...+w_dx_d+b$ ，简洁表示就是 $y_hat = W^TX+b$ 
这里给定 X 和 Y 之后，线性回归的目标是寻找到一组权重向量 w 和 b，使得给定新的 X 的时候，预测的 y 尽可能少出现误差。

寻找最优的模型参数需要两个东西
+ 模型质量度量方式
+ 一个能使模型更新以提升模型预测质量的方法

### 3.1.1.2 损失函数
损失函数就是模型拟合度量，量化目标的实际值和预测值之间的差距。通常选择非负的作为损失，数值越小，损失越小，模型越优。回归问题常用的损失函数是平方误差函数，公式如下： $l^{(i)}_{w,b}=\frac{1}{2}(y_hat^{(i)}-y^{(i)})^2$ 
上面损失函数中，估计值和观测值之间较大的差异导致更大的损失。为了度量模型在整个数据集上质量，我们需要计算在训练 n 个样本上的损失值： $L(w,b)=\frac{1}{n}\sum_{i=1}^n{l^{(i)}_{w,b}}$
在训练模型的时候，我们希望寻找一组参数 $(w^*, b^*)$ ， 这组参数能最小化所有样本的总损失。
 $w^*,b^*=argmin_{w,b}L(w,b)$ 

### 3.1.1.3 解析解
线性回归刚好是一个简单的优化问题。线性回归的解可以用一个公式简单的表达出来，这类解叫做解析解。将偏置 b 合并到参数 w 中。我们预测问题是最小化 $||y-XW||^2$ 。这个损失平面只有一个零界点，这个点对应于整个区域的损失极小点。将损失关于 w 的导数设为0， 得到解析解：
 $w^*=(X^TX)^{-1}X^Ty$

### 3.1.1.4 随机梯度下降
数学公式表示
 $(w,b) <- (w,b) - \frac{\eta}{|\beta|}\sum_{i \in \beta}{\alpha_{w,b}l^{(i)}(w,b)}$
线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为泛化（generalization）。

## 3.1.2 矢量化加速
训练模型时，我们希望能够同时处理整个小批量样本。为了实现这一点，我们对计算进行矢量化，从而利用线性代数库，而不是用 python 的 for 循环

```python
class Timer:  #@save
    """记录多次运行时间"""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """启动计时器"""
        self.tik = time.time()

    def stop(self):
        """停止计时器并将时间记录在列表中"""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """返回平均时间"""
        return sum(self.times) / len(self.times)

    def sum(self):
        """返回时间总和"""
        return sum(self.times)

    def cumsum(self):
        """返回累计时间"""
        return np.array(self.times).cumsum().tolist()
```
```python
c = tf.Variable(tf.zeros(n))
timer = Timer()
for i in range(n):
    c[i].assign(a[i] + b[i])
f'time.stop():.5f} sec'
# output
'7.33347 sec'
```

我们使用重载的+运算符来计算按元素的和
```python 
timer.start()
d = a + b
f'{timer.stop(): .5f} sec'
# output
' 0.00100 sec'
```
很明显，矢量化代码通常会带来数量级的加速。

## 3.1.3 正态分布和平方损失
正太分布和线性回归之间的关系很密切。正太分布也叫高斯分布。简单来讲，若随机变量 x 具有均值 $\mu$ 和方差 $\sigma ^ 2$ ,其正态分布概率密度表示如下： $p(x)=\frac{1}{\sqrt{2\omicron\sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$

```python
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma ** 2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
x = np.arange(-7, 7, 0.01)
# mu and sigma
params = [(0, 1), (0, 2), (3,1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x', ylabel='p(x)', 
         figsize=(4.5,2.5), legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```
改变均值会产生沿轴的偏移，增加方差将会分散分布、降低其峰值。
均方误差损失函数可以用于回归的一个原因是：我们假设了观测中包含噪声，其中噪声服从正态分布。噪声正态分布如下： $y=w^Tx+b+\epsilon$ 其中 $\epsilon ~ N(0, \sigma^2)$.
因此我们可以写出通过给定 x 观测到特定 y 的似然：$p(y|X)=\frac{1}{\sqrt{2\omicron\sigma^2}}exp(-\frac{1}{2\sigma^2}(y-w^Tx-b)^2)$
