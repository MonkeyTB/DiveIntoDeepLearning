+ 写在前面
机器学习就是做出预测，这个预测就是所谓的概率

## 2.6.1 基本概率论
掷筛子，看到1的概率有多大？ $1/6$ 
现实生活中，工厂的筛子需要检查是否瑕疵。检查的手段就是多次投掷并记录结果。观察1-6出现的次数。大数定律告诉我们，随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。
```python
%mapplotlib inline
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
from d2l import tensorflow as d2l
```

统计学中，我们把从概率分布中抽取样本的过程称为抽样（sampling）。笼统来说，可以把分布（distribution）看作事件的概率分配。将概率分配给一些离散选择的分布称为多项分布（multinomial distribution）。
```python
fair_probs = tf.ones(6) / 6
tfp.distributions.Multinomial(1, fair_probs).sample()
# output
<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 1., 0., 0., 0.], dtype=float32)>
```
为了公平性，我们希望同一个分布中生成多个样本。如果用 python 的 for 循环来完成，速度会很慢。
```python
tfp.distributions.Multinomial(10, fair_probs).sample()
tfp.distributions.Multinomial(1000, fair_probs).sample()
# output
<tf.Tensor: shape=(6,), dtype=float32, numpy=array([3., 2., 1., 0., 2., 2.], dtype=float32)>
<tf.Tensor: shape=(6,), dtype=float32, numpy=array([163., 173., 179., 160., 170., 155.], dtype=float32)>
```
一个公平的股子中每个结果都用过有概率 $1/6$ ， 大约0.167.
我们看这些概率如何随着时间的推移收敛到真实概率。进行500组实验，每组抽取10个样本。
```python
counts = tfp.distributions.Multinomial(10, fair_probs).sample(500)
cum_counts = tf.cumsum(counts, axis=0) # 按列累加【a, a+b, a+b+c, ...】
estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)

d2l.set_figsize((6,4.5))
for i in range(6):
    d2l.plt.plot(estimates[:, i].numpy(), label=("P(die="+str(i+1)+")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Groups of experiments')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend()
```
每条实线对应于骰子的6个值中的一个，并给出骰子在每组实验后出现值的估计概率。 当我们通过更多的实验获得更多的数据时，这条实体曲线向真实概率收敛。

### 2.6.1.1 概率化公式
上面例子中，我们将集合 $S={1,2,3,4,5,6}$ 称为样本空间或结果空间，其中每个元素都是结果。事件（event）是一组给定样本空间的随机结果。
概率（probability）可以被认为是将集合映射到真实值的函数，在给定样本空间 S 中，事件 A 的概率，表示为 $P(A)$ ，满足以下属性：
+ 对于任意事件 A， $P(A) \geq 0$
+ 整个样本空间的概率为1
+ 对于互斥事件的任意一个可能序列 $A_1,A_2, ...$ ，序列中任意一个事件发生的概率等于他们各自概率之和。

### 2.6.1.2 随机变量
随机变量几乎可以是任何数值。注意离散（discrete）随机变量和连续（continuous）随机变量存在微妙的区别，连续随机变量某个值的概率可能没有意义，但某个区间的概率却很有意义，我们称为密度（density）

## 2.6.2 处理多个随机变量
很多时候，会考虑多个随机变量。比如我们需要对疾病和症状之间的关系建模。

### 2.6.2.1 联合概率
 $P(A=a, B=b) = P(A=a)*P(B=b)$ ，联合概率发生的可能性不大于单独发生的可能性

### 2.6.2.2 条件概率
联合概率不等式给我们一个有趣的比率： $0 \leq \frac{P(A=a,B=b)}{P(A=a)} \leq 1$ ，我们称这个比率为条件概率（conditional probability）。并用 $P(B=b|A=a)$ 表示 A=a发生的情况下 B=b发生的概率

### 2.6.2.3 贝叶斯定理
 $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$

### 2.6.2.4 独立性
依赖（dependence）和独立（independence）。如果两个随机事件 A 和 B 是独立的，那么 A 发生跟 B发生没有关系

## 2.6.3 期望和方差
概率的测量方法——期望（expectation）和方差（standard deviation）。
期望其实就是平均值 $E[x]=\sum_x{xP(X=x)}$
当函数 $f(x)$ 的输入是从分布 P 中抽取的随机变量时， $f(x)$ 的期望值为： $E_{x~p}[f(x)]=\sum_x{f(x)P(x)}$
许多情况下，我们希望衡量随机变量 X 与其期望的偏置，可以通过方差来量化。
 $Var[X]=E[(X-E[X])^2]=E[X^2]-E[X]^2$
 方差的平方根被称为标准差（standard deviation）。随机变量函数的方差衡量是：当从该随机分布中采样不同值 x 时，函数值偏离该函数期望的程度： $ Var[f(x)]=E[(f(x)-E[f(x)])^2]$ 