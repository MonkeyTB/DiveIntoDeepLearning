+ 写在前面
```python
import tensorflow as tf
net = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10)
])
X = tf.random.uniform((2,20))
net(X)
```

通过实例化 keras.models.Sequential 来构建模型，层的执行顺序是作为参数传递的。 Sequential 定义了一种特殊的 keras.Model ， 即在 Keras 中表示一个块的类。  它维护了一个由Model组成的有序列表， 注意两个全连接层都是Model类的实例， 这个类本身就是Model的子类。 前向传播（call）函数也非常简单： 它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。注意，到目前为止，我们一直在通过 net(X) 调用我们的模型来获得模型的输出。 这实际上是 net.call(X) 的简写， 这是通过 Block 类的 __call__ 函数实现的一个Python技巧。

## 5.1.1 自定义块

1、将输入数据作为其前向传播函数的参数
2、通过前向传播函数来生成输出。请注意，输出的形状可能与输入不同。
3、计算其输出关于输入的梯度，可通过其反向传播函数进行访问。
4、存储和访问前向传播计算所需的参数
5、根据需要初始化模型参数

在下面的代码片段中，我们从零开始编写一个块。 它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。 注意，下面的 MLP 类继承了表示块的类。 我们的实现只需要提供我们自己的构造函数（Python中的__init__函数）和前向传播函数。

```python
class MLP(tf.keras.Model):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Model的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = tf.keras.layer.Dense(units=256, activation=tf.nn.relu)
        self.out = tf.keras.layers.Dense(units=10)

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def call(self, x):
        return self.out(self.hidden(x))
```

我们首先看一下前向传播函数，它以 X 作为输入， 计算带有激活函数的隐藏表示，并输出其未规范化的输出值。 在这个MLP实现中，两个层都是实例变量。 要了解这为什么是合理的，可以想象实例化两个多层感知机（net1和net2）， 并根据不同的数据对它们进行训练。 当然，我们希望它们学到两种不同的模型。

接着我们实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层。 注意一些关键细节： 首先，我们定制的__init__函数通过super().__init__() 调用父类的__init__函数， 省去了重复编写模版代码的痛苦。 然后，我们实例化两个全连接层，分别为self.hidden和self.out。注意，除非我们实现一个新的运算符， 否则我们不必担心反向传播函数或参数初始化，系统将自动生成这些。

## 5.1.2 顺序块

现在我们可以更仔细地看看 Sequential 类是如何工作的， 回想一下 Sequential 的设计是为了把其他模块串起来。 为了构建我们自己的简化的 MySequential ， 我们只需要定义两个关键函数：

一种将块逐个追加到列表中的函数；

一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。

下面的 MySequential 类提供了与默认 Sequential 类相同的功能。

```python
class MySequential(tf.keras.Model):
    def __init__(self, *args):
        super().__init__()
        self.modules = []
        for block in args:
            self.modules.append(block)
    
    def call(self, x):
        for module in self.modules:
            X = module(X)
        return X
```
当 MySequential 的前向传播函数被调用时， 每个添加的块都按照它们被添加的顺序执行。 现在可以使用我们的MySequential类重新实现多层感知机。

```python
net = MySequential(
    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),
    tf.keras.layers.Dense(units=10)
)
net(X)
```

## 5.1.3 在前向传播函数中执行代码
Sequential类使模型构造变得简单， 允许我们组合新的架构，而不必定义自己的类。 然而，并不是所有的架构都是简单的顺序架构。 当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算， 而不是简单地依赖预定义的神经网络层。

到目前为止， 我们网络中的所有操作都对网络的激活值及网络的参数起作用。 然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项， 我们称之为常数参数（constant parameter）。 例如，我们需要一个计算函数 $f(x,w)=c·w^Tx$ 的层， 其中 x 是输入， w 是参数， c 是某个在优化过程中没有更新的指定常量。 因此我们实现了一个FixedHiddenMLP类，如下所示：

```python
class FixedHiddenMLP(tf.keras.Model):
    def __init__(self):
        supuer().__init__()
        self.flatten = tf.keras.layers.Flatten()
        # 使用tf.constant函数创建的随机权重参数在训练期间不会更新（即为常量参数）
        self.rand_weight = tf.constant(tf.random.uniform((20,20)))
        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)
    
    def call(self, inputs):
        X = self.flatten(inputs)
        # 使用创建的常量参数以及relu和matmul函数
        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数。
        x = self.dense(x)
        # 控制流
        while tf.reduce_sum(tf.math.abs(X)) > 1:
            X /= 2
        return tf.reduce_sum(X)

```
在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。

注意，在返回输出之前，模型做了一些不寻常的事情： 它运行了一个while循环，在 L1 范数大于 1 的条件下， 将输出向量除以 2 ，直到它满足条件为止。 最后，模型返回了X中所有项的和。 注意，此操作可能不会常用于在任何实际任务中， 我们只展示如何将任意代码集成到神经网络计算的流程中。

我们可以混合搭配各种组合块的方法。
```python
class NestMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.net = tf.keras.Sequential()
        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))
        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))
        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)

    def call(self, inputs):
        return self.dense(self.net(inputs))

chimera = tf.keras.Sequential()
chimera.add(NestMLP)
chimera.add(tf.keras.layers.Dense(20))
chimera.add(FixedHiddenMLP())
chimera(X)
```